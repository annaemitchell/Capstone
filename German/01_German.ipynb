{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project, by Anna Mitchell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation\n",
    "### English to German "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code help and adaption for this project come from the following resources:\n",
    "\n",
    "#https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/\n",
    "#https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
    "#https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "#https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/\n",
    "#https://google.github.io/seq2seq/nmt/\n",
    "#https://opennmt.net/\n",
    "#https://stackoverflow.com/questions/tagged/machine-translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "# load data to preserve unicode german characters, loads file as a blob of text\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a loaded document into sentences\n",
    "def pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "# remove non printable characters\n",
    "# remove all punctuation characters\n",
    "# normalize all unicode characters to ASCII\n",
    "#normalize the case to lower\n",
    "#remove any remaining tokens that are not alphabetic \n",
    "#perform these operations on each phrase for each pair in the loaded dataset.\n",
    "\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Source: http://www.manythings.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german.pkl\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "filename = 'deu1.txt'\n",
    "doc = load_doc(filename)\n",
    "# split into english-german pairs\n",
    "pairs = pairs(doc)\n",
    "# clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "# save clean pairs to file\n",
    "clean_data(clean_pairs, 'english-german.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[go] => [geh]\n",
      "[hi] => [hallo]\n",
      "[hi] => [gru gott]\n",
      "[run] => [lauf]\n",
      "[run] => [lauf]\n",
      "[wow] => [potzdonner]\n",
      "[wow] => [donnerwetter]\n",
      "[fire] => [feuer]\n",
      "[help] => [hilfe]\n",
      "[help] => [zu hulf]\n",
      "[stop] => [stopp]\n",
      "[wait] => [warte]\n",
      "[wait] => [warte]\n",
      "[begin] => [fang an]\n",
      "[go on] => [mach weiter]\n",
      "[hello] => [hallo]\n",
      "[hurry] => [beeil dich]\n",
      "[hurry] => [schnell]\n",
      "[i hid] => [ich versteckte mich]\n",
      "[i hid] => [ich habe mich versteckt]\n",
      "[i ran] => [ich rannte]\n",
      "[i see] => [ich verstehe]\n",
      "[i see] => [aha]\n",
      "[i try] => [ich probiere es]\n",
      "[i won] => [ich hab gewonnen]\n",
      "[i won] => [ich habe gewonnen]\n",
      "[relax] => [entspann dich]\n",
      "[shoot] => [feuer]\n",
      "[shoot] => [schie]\n",
      "[smile] => [lacheln]\n",
      "[ask me] => [frag mich]\n",
      "[ask me] => [fragt mich]\n",
      "[ask me] => [fragen sie mich]\n",
      "[attack] => [angriff]\n",
      "[attack] => [attacke]\n",
      "[cheers] => [zum wohl]\n",
      "[eat it] => [iss es]\n",
      "[eat up] => [iss auf]\n",
      "[eat up] => [iss auf]\n",
      "[freeze] => [keine bewegung]\n",
      "[freeze] => [stehenbleiben]\n",
      "[go now] => [geh jetzt]\n",
      "[got it] => [verstanden]\n",
      "[got it] => [aha]\n",
      "[got it] => [ich habs]\n",
      "[got it] => [kapiert]\n",
      "[got it] => [verstanden]\n",
      "[got it] => [einverstanden]\n",
      "[he ran] => [er rannte]\n",
      "[he ran] => [er lief]\n"
     ]
    }
   ],
   "source": [
    "# check if working\n",
    "for i in range(50):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german-both.pkl\n",
      "Saved: english-german-train.pkl\n",
      "Saved: english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('english-german.pkl')\n",
    "\n",
    "# reduce dataset size\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:9000], dataset[9000:]\n",
    "# save\n",
    "clean_data(dataset, 'english-german-both.pkl')\n",
    "clean_data(train, 'english-german-train.pkl')\n",
    "clean_data(test, 'english-german-test.pkl')\n",
    "\n",
    "#both.pkl - contains all of the train and test examples that we can use to \n",
    "#-> define the parameters of the problem, such a smax phrase lengths and the vocab\n",
    "# train.pkl and test.pkl - train and test dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text to Sequence Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read raw text file\n",
    "def read_text(filename):\n",
    "    # open the file\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a text into sentences\n",
    "def to_lines(text):\n",
    "    sents = text.strip().split('\\n')\n",
    "    sents = [i.split('\\t') for i in sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_text(\"deu1.txt\")\n",
    "ger_eng = to_lines(data)\n",
    "ger_eng = array(ger_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_eng = ger_eng[:50000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "ger_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in ger_eng[:,0]]\n",
    "ger_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in ger_eng[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lowercase\n",
    "for i in range(len(ger_eng)):\n",
    "    ger_eng[i,0] = ger_eng[i,0].lower()\n",
    "    \n",
    "    ger_eng[i,1] = ger_eng[i,1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists\n",
    "eng_l = []\n",
    "ger_l = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in ger_eng[:,0]:\n",
    "    eng_l.append(len(i.split()))\n",
    "\n",
    "for i in ger_eng[:,1]:\n",
    "    ger_l.append(len(i.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df = pd.DataFrame({'english':eng_l, 'german':ger_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximum length of German: 11\n",
    "#Maxiumum lenght of English: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build a tokenizer\n",
    "def tokenization(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 6243\n"
     ]
    }
   ],
   "source": [
    "# english tokenizer\n",
    "eng_tokenizer = tokenization(ger_eng[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "eng_length = 8\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German Vocabulary Size: 10329\n"
     ]
    }
   ],
   "source": [
    "# German tokenizer\n",
    "ger_tokenizer = tokenization(ger_eng[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "\n",
    "ger_length = 8\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimizer: rmsprop \n",
    "* Loss: categorical_crossentropy\n",
    "* Epochs: 30\n",
    "* Batch size: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#develop neural translation model\n",
    "#load and prepare clean text data ready for modeling and define and train the model\n",
    "#-> on the prepared data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code adapted from https://medium.com/@umerfarooq_26378/neural-machine-translation-with-code-68c425044bbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2241\n",
      "English Max Length: 5\n",
      "German Vocabulary Size: 3572\n",
      "German Max Length: 9\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 9, 256)            914432    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 5, 2241)           575937    \n",
      "=================================================================\n",
      "Total params: 2,540,993\n",
      "Trainable params: 2,540,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.36010, saving model to model.h5\n",
      "141/141 - 9s - loss: 4.1092 - accuracy: 0.4520 - val_loss: 3.3601 - val_accuracy: 0.4748\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.36010 to 3.22749, saving model to model.h5\n",
      "141/141 - 8s - loss: 3.2078 - accuracy: 0.4819 - val_loss: 3.2275 - val_accuracy: 0.4856\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.22749 to 3.14786, saving model to model.h5\n",
      "141/141 - 8s - loss: 3.0682 - accuracy: 0.4934 - val_loss: 3.1479 - val_accuracy: 0.4914\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.14786 to 3.00237, saving model to model.h5\n",
      "141/141 - 8s - loss: 2.9256 - accuracy: 0.5087 - val_loss: 3.0024 - val_accuracy: 0.5116\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.00237 to 2.89640, saving model to model.h5\n",
      "141/141 - 8s - loss: 2.7595 - accuracy: 0.5271 - val_loss: 2.8964 - val_accuracy: 0.5292\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.89640 to 2.79939, saving model to model.h5\n",
      "141/141 - 8s - loss: 2.6155 - accuracy: 0.5459 - val_loss: 2.7994 - val_accuracy: 0.5424\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.79939 to 2.71499, saving model to model.h5\n",
      "141/141 - 8s - loss: 2.4760 - accuracy: 0.5659 - val_loss: 2.7150 - val_accuracy: 0.5576\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.71499 to 2.61137, saving model to model.h5\n",
      "141/141 - 8s - loss: 2.3457 - accuracy: 0.5812 - val_loss: 2.6114 - val_accuracy: 0.5754\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.61137 to 2.51923, saving model to model.h5\n",
      "141/141 - 8s - loss: 2.2078 - accuracy: 0.6002 - val_loss: 2.5192 - val_accuracy: 0.5902\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.51923 to 2.42930, saving model to model.h5\n",
      "141/141 - 8s - loss: 2.0735 - accuracy: 0.6154 - val_loss: 2.4293 - val_accuracy: 0.5982\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.42930 to 2.37121, saving model to model.h5\n",
      "141/141 - 8s - loss: 1.9418 - accuracy: 0.6299 - val_loss: 2.3712 - val_accuracy: 0.6082\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.37121 to 2.29943, saving model to model.h5\n",
      "141/141 - 8s - loss: 1.8184 - accuracy: 0.6459 - val_loss: 2.2994 - val_accuracy: 0.6154\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.29943 to 2.21989, saving model to model.h5\n",
      "141/141 - 8s - loss: 1.7027 - accuracy: 0.6600 - val_loss: 2.2199 - val_accuracy: 0.6304\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.21989 to 2.17176, saving model to model.h5\n",
      "141/141 - 8s - loss: 1.5901 - accuracy: 0.6759 - val_loss: 2.1718 - val_accuracy: 0.6352\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.17176 to 2.12355, saving model to model.h5\n",
      "141/141 - 8s - loss: 1.4872 - accuracy: 0.6903 - val_loss: 2.1236 - val_accuracy: 0.6484\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.12355 to 2.08348, saving model to model.h5\n",
      "141/141 - 8s - loss: 1.3831 - accuracy: 0.7068 - val_loss: 2.0835 - val_accuracy: 0.6568\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.08348 to 2.05234, saving model to model.h5\n",
      "141/141 - 8s - loss: 1.2868 - accuracy: 0.7216 - val_loss: 2.0523 - val_accuracy: 0.6598\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.05234 to 2.01397, saving model to model.h5\n",
      "141/141 - 8s - loss: 1.1923 - accuracy: 0.7370 - val_loss: 2.0140 - val_accuracy: 0.6660\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.01397 to 1.98334, saving model to model.h5\n",
      "141/141 - 8s - loss: 1.1067 - accuracy: 0.7531 - val_loss: 1.9833 - val_accuracy: 0.6668\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.98334 to 1.93694, saving model to model.h5\n",
      "141/141 - 8s - loss: 1.0178 - accuracy: 0.7686 - val_loss: 1.9369 - val_accuracy: 0.6812\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.93694 to 1.93170, saving model to model.h5\n",
      "141/141 - 8s - loss: 0.9368 - accuracy: 0.7874 - val_loss: 1.9317 - val_accuracy: 0.6810\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.93170 to 1.90096, saving model to model.h5\n",
      "141/141 - 8s - loss: 0.8612 - accuracy: 0.8018 - val_loss: 1.9010 - val_accuracy: 0.6820\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.90096 to 1.89837, saving model to model.h5\n",
      "141/141 - 8s - loss: 0.7929 - accuracy: 0.8160 - val_loss: 1.8984 - val_accuracy: 0.6852\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.89837 to 1.86756, saving model to model.h5\n",
      "141/141 - 8s - loss: 0.7263 - accuracy: 0.8333 - val_loss: 1.8676 - val_accuracy: 0.6972\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.86756 to 1.85190, saving model to model.h5\n",
      "141/141 - 8s - loss: 0.6656 - accuracy: 0.8460 - val_loss: 1.8519 - val_accuracy: 0.6928\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.85190 to 1.84896, saving model to model.h5\n",
      "141/141 - 8s - loss: 0.6063 - accuracy: 0.8608 - val_loss: 1.8490 - val_accuracy: 0.7004\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.84896 to 1.83573, saving model to model.h5\n",
      "141/141 - 8s - loss: 0.5557 - accuracy: 0.8708 - val_loss: 1.8357 - val_accuracy: 0.7048\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.83573 to 1.83056, saving model to model.h5\n",
      "141/141 - 8s - loss: 0.5062 - accuracy: 0.8840 - val_loss: 1.8306 - val_accuracy: 0.7002\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.83056 to 1.82371, saving model to model.h5\n",
      "141/141 - 8s - loss: 0.4629 - accuracy: 0.8925 - val_loss: 1.8237 - val_accuracy: 0.7066\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.82371\n",
      "141/141 - 8s - loss: 0.4236 - accuracy: 0.9015 - val_loss: 1.8260 - val_accuracy: 0.7110\n"
     ]
    }
   ],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    " \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    " \n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    " \n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    " \n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    " \n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    #core layer, encodes the input by indices \n",
    "    #If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1) per keras library.\n",
    "    model.add(LSTM(n_units))\n",
    "    #recurrent layer to choose different implementation to maximize performance \n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    #reshaping layer to repeat the input by a specific amount of times \n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "     #recurrent layer to choose different implementation to maximize performance \n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    #activation layer, wrapper to apply a layer to every temporal slice of an input \n",
    "    return model\n",
    " \n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    " \n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n",
    " \n",
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    " \n",
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history = model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.36544615030288696, 0.9199555516242981]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(trainX, trainY, verbose=0)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy of 92%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(loss) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the training and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#clear the figure, palette cleanser, if you will :)\n",
    "plt.clf();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimizer: Adam \n",
    "* Loss: categorical_crossentropy\n",
    "* Epochs: 10\n",
    "* Batch size: 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2241\n",
      "English Max Length: 5\n",
      "German Vocabulary Size: 3572\n",
      "German Max Length: 9\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 9, 256)            914432    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 5, 2241)           575937    \n",
      "=================================================================\n",
      "Total params: 2,540,993\n",
      "Trainable params: 2,540,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.40185, saving model to model.h5\n",
      "113/113 - 8s - loss: 4.2968 - accuracy: 0.4457 - val_loss: 3.4018 - val_accuracy: 0.4674\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.40185 to 3.24821, saving model to model.h5\n",
      "113/113 - 8s - loss: 3.2621 - accuracy: 0.4771 - val_loss: 3.2482 - val_accuracy: 0.4814\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.24821 to 3.18525, saving model to model.h5\n",
      "113/113 - 7s - loss: 3.1225 - accuracy: 0.4873 - val_loss: 3.1852 - val_accuracy: 0.4868\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.18525 to 3.12974, saving model to model.h5\n",
      "113/113 - 7s - loss: 3.0220 - accuracy: 0.4955 - val_loss: 3.1297 - val_accuracy: 0.4928\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.12974 to 3.01949, saving model to model.h5\n",
      "113/113 - 7s - loss: 2.9002 - accuracy: 0.5134 - val_loss: 3.0195 - val_accuracy: 0.5132\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.01949 to 2.91321, saving model to model.h5\n",
      "113/113 - 7s - loss: 2.7601 - accuracy: 0.5259 - val_loss: 2.9132 - val_accuracy: 0.5274\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.91321 to 2.81564, saving model to model.h5\n",
      "113/113 - 7s - loss: 2.6219 - accuracy: 0.5466 - val_loss: 2.8156 - val_accuracy: 0.5480\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.81564 to 2.72723, saving model to model.h5\n",
      "113/113 - 7s - loss: 2.4923 - accuracy: 0.5638 - val_loss: 2.7272 - val_accuracy: 0.5546\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.72723 to 2.63947, saving model to model.h5\n",
      "113/113 - 7s - loss: 2.3688 - accuracy: 0.5782 - val_loss: 2.6395 - val_accuracy: 0.5672\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.63947 to 2.58313, saving model to model.h5\n",
      "113/113 - 7s - loss: 2.2509 - accuracy: 0.5932 - val_loss: 2.5831 - val_accuracy: 0.5770\n"
     ]
    }
   ],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    " \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    " \n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    " \n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    " \n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    " \n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    #core layer, encodes the input by indices \n",
    "    #If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1) per keras library.\n",
    "    model.add(LSTM(n_units))\n",
    "    #recurrent layer to choose different implementation to maximize performance \n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    #reshaping layer to repeat the input by a specific amount of times \n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "     #recurrent layer to choose different implementation to maximize performance \n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    #activation layer, wrapper to apply a layer to every temporal slice of an input \n",
    "    return model\n",
    " \n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    " \n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n",
    " \n",
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    " \n",
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history2 = model.fit(trainX, trainY, epochs=10, batch_size=80, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.1415884494781494, 0.6006222367286682]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(trainX, trainY, verbose=0)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accurracy is way lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimizer: rmsprop \n",
    "* Loss: categorical_crossentropy\n",
    "* Epochs: 50\n",
    "* Batch Size: 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2241\n",
      "English Max Length: 5\n",
      "German Vocabulary Size: 3572\n",
      "German Max Length: 9\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 9, 256)            914432    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_5 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 5, 2241)           575937    \n",
      "=================================================================\n",
      "Total params: 2,540,993\n",
      "Trainable params: 2,540,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.32308, saving model to model.h5\n",
      "180/180 - 11s - loss: 3.9845 - accuracy: 0.4542 - val_loss: 3.3231 - val_accuracy: 0.4744\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.32308 to 3.20190, saving model to model.h5\n",
      "180/180 - 9s - loss: 3.1861 - accuracy: 0.4840 - val_loss: 3.2019 - val_accuracy: 0.4890\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.20190 to 3.06746, saving model to model.h5\n",
      "180/180 - 9s - loss: 3.0237 - accuracy: 0.4956 - val_loss: 3.0675 - val_accuracy: 0.4940\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.06746 to 2.92032, saving model to model.h5\n",
      "180/180 - 9s - loss: 2.8342 - accuracy: 0.5197 - val_loss: 2.9203 - val_accuracy: 0.5228\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.92032 to 2.81819, saving model to model.h5\n",
      "180/180 - 9s - loss: 2.6668 - accuracy: 0.5388 - val_loss: 2.8182 - val_accuracy: 0.5440\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.81819 to 2.70388, saving model to model.h5\n",
      "180/180 - 8s - loss: 2.5059 - accuracy: 0.5633 - val_loss: 2.7039 - val_accuracy: 0.5590\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.70388 to 2.58578, saving model to model.h5\n",
      "180/180 - 9s - loss: 2.3496 - accuracy: 0.5823 - val_loss: 2.5858 - val_accuracy: 0.5726\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.58578 to 2.48938, saving model to model.h5\n",
      "180/180 - 8s - loss: 2.1966 - accuracy: 0.5982 - val_loss: 2.4894 - val_accuracy: 0.5860\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.48938 to 2.41359, saving model to model.h5\n",
      "180/180 - 8s - loss: 2.0519 - accuracy: 0.6160 - val_loss: 2.4136 - val_accuracy: 0.5984\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.41359 to 2.32291, saving model to model.h5\n",
      "180/180 - 8s - loss: 1.9125 - accuracy: 0.6332 - val_loss: 2.3229 - val_accuracy: 0.6124\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.32291 to 2.26633, saving model to model.h5\n",
      "180/180 - 8s - loss: 1.7756 - accuracy: 0.6513 - val_loss: 2.2663 - val_accuracy: 0.6176\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.26633 to 2.20204, saving model to model.h5\n",
      "180/180 - 8s - loss: 1.6550 - accuracy: 0.6680 - val_loss: 2.2020 - val_accuracy: 0.6316\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.20204 to 2.15638, saving model to model.h5\n",
      "180/180 - 8s - loss: 1.5369 - accuracy: 0.6849 - val_loss: 2.1564 - val_accuracy: 0.6386\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.15638 to 2.09709, saving model to model.h5\n",
      "180/180 - 8s - loss: 1.4286 - accuracy: 0.6995 - val_loss: 2.0971 - val_accuracy: 0.6502\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.09709 to 2.06712, saving model to model.h5\n",
      "180/180 - 8s - loss: 1.3237 - accuracy: 0.7164 - val_loss: 2.0671 - val_accuracy: 0.6524\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.06712 to 2.02166, saving model to model.h5\n",
      "180/180 - 8s - loss: 1.2209 - accuracy: 0.7318 - val_loss: 2.0217 - val_accuracy: 0.6646\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.02166 to 1.98264, saving model to model.h5\n",
      "180/180 - 8s - loss: 1.1236 - accuracy: 0.7496 - val_loss: 1.9826 - val_accuracy: 0.6690\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.98264 to 1.94854, saving model to model.h5\n",
      "180/180 - 8s - loss: 1.0321 - accuracy: 0.7654 - val_loss: 1.9485 - val_accuracy: 0.6716\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.94854 to 1.92665, saving model to model.h5\n",
      "180/180 - 8s - loss: 0.9426 - accuracy: 0.7829 - val_loss: 1.9267 - val_accuracy: 0.6766\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.92665 to 1.89559, saving model to model.h5\n",
      "180/180 - 8s - loss: 0.8587 - accuracy: 0.8007 - val_loss: 1.8956 - val_accuracy: 0.6878\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.89559 to 1.86503, saving model to model.h5\n",
      "180/180 - 9s - loss: 0.7798 - accuracy: 0.8184 - val_loss: 1.8650 - val_accuracy: 0.6922\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.86503 to 1.86386, saving model to model.h5\n",
      "180/180 - 9s - loss: 0.7085 - accuracy: 0.8346 - val_loss: 1.8639 - val_accuracy: 0.6946\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.86386 to 1.83717, saving model to model.h5\n",
      "180/180 - 10s - loss: 0.6404 - accuracy: 0.8510 - val_loss: 1.8372 - val_accuracy: 0.7004\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.83717 to 1.83273, saving model to model.h5\n",
      "180/180 - 10s - loss: 0.5789 - accuracy: 0.8651 - val_loss: 1.8327 - val_accuracy: 0.7036\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.83273\n",
      "180/180 - 9s - loss: 0.5208 - accuracy: 0.8784 - val_loss: 1.8444 - val_accuracy: 0.6996\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.83273 to 1.82189, saving model to model.h5\n",
      "180/180 - 9s - loss: 0.4741 - accuracy: 0.8877 - val_loss: 1.8219 - val_accuracy: 0.7096\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.82189 to 1.81860, saving model to model.h5\n",
      "180/180 - 9s - loss: 0.4285 - accuracy: 0.8987 - val_loss: 1.8186 - val_accuracy: 0.7104\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.81860\n",
      "180/180 - 9s - loss: 0.3873 - accuracy: 0.9072 - val_loss: 1.8209 - val_accuracy: 0.7080\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.81860\n",
      "180/180 - 9s - loss: 0.3509 - accuracy: 0.9161 - val_loss: 1.8227 - val_accuracy: 0.7170\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.81860\n",
      "180/180 - 8s - loss: 0.3213 - accuracy: 0.9219 - val_loss: 1.8257 - val_accuracy: 0.7118\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.81860\n",
      "180/180 - 8s - loss: 0.2916 - accuracy: 0.9283 - val_loss: 1.8261 - val_accuracy: 0.7162\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.81860\n",
      "180/180 - 9s - loss: 0.2680 - accuracy: 0.9330 - val_loss: 1.8321 - val_accuracy: 0.7140\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.81860 to 1.81764, saving model to model.h5\n",
      "180/180 - 9s - loss: 0.2465 - accuracy: 0.9373 - val_loss: 1.8176 - val_accuracy: 0.7188\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.81764\n",
      "180/180 - 9s - loss: 0.2289 - accuracy: 0.9401 - val_loss: 1.8541 - val_accuracy: 0.7138\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.81764\n",
      "180/180 - 9s - loss: 0.2119 - accuracy: 0.9430 - val_loss: 1.8478 - val_accuracy: 0.7174\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.81764\n",
      "180/180 - 9s - loss: 0.1961 - accuracy: 0.9448 - val_loss: 1.8471 - val_accuracy: 0.7174\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.81764\n",
      "180/180 - 9s - loss: 0.1825 - accuracy: 0.9465 - val_loss: 1.8575 - val_accuracy: 0.7198\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.81764\n",
      "180/180 - 9s - loss: 0.1731 - accuracy: 0.9484 - val_loss: 1.8655 - val_accuracy: 0.7166\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.81764\n",
      "180/180 - 8s - loss: 0.1640 - accuracy: 0.9494 - val_loss: 1.8780 - val_accuracy: 0.7152\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.81764\n",
      "180/180 - 9s - loss: 0.1557 - accuracy: 0.9494 - val_loss: 1.8943 - val_accuracy: 0.7170\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.81764\n",
      "180/180 - 9s - loss: 0.1482 - accuracy: 0.9497 - val_loss: 1.8904 - val_accuracy: 0.7154\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.81764\n",
      "180/180 - 9s - loss: 0.1411 - accuracy: 0.9522 - val_loss: 1.8804 - val_accuracy: 0.7212\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.81764\n",
      "180/180 - 9s - loss: 0.1354 - accuracy: 0.9526 - val_loss: 1.9009 - val_accuracy: 0.7194\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.81764\n",
      "180/180 - 8s - loss: 0.1325 - accuracy: 0.9522 - val_loss: 1.9077 - val_accuracy: 0.7214\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.81764\n",
      "180/180 - 9s - loss: 0.1284 - accuracy: 0.9512 - val_loss: 1.9248 - val_accuracy: 0.7174\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.81764\n",
      "180/180 - 8s - loss: 0.1224 - accuracy: 0.9532 - val_loss: 1.9256 - val_accuracy: 0.7206\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.81764\n",
      "180/180 - 8s - loss: 0.1201 - accuracy: 0.9523 - val_loss: 1.9409 - val_accuracy: 0.7238\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.81764\n",
      "180/180 - 8s - loss: 0.1160 - accuracy: 0.9530 - val_loss: 1.9428 - val_accuracy: 0.7200\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.81764\n",
      "180/180 - 9s - loss: 0.1121 - accuracy: 0.9538 - val_loss: 1.9529 - val_accuracy: 0.7188\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.81764\n",
      "180/180 - 9s - loss: 0.1113 - accuracy: 0.9530 - val_loss: 1.9614 - val_accuracy: 0.7230\n"
     ]
    }
   ],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    " \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    " \n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    " \n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    " \n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    " \n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    #core layer, encodes the input by indices \n",
    "    #If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1) per keras library.\n",
    "    model.add(LSTM(n_units))\n",
    "    #recurrent layer to choose different implementation to maximize performance \n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    #reshaping layer to repeat the input by a specific amount of times \n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "     #recurrent layer to choose different implementation to maximize performance \n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    #activation layer, wrapper to apply a layer to every temporal slice of an input \n",
    "    return model\n",
    " \n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    " \n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n",
    " \n",
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    " \n",
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history2 = model.fit(trainX, trainY, epochs=50, batch_size=50, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0913555771112442, 0.9636889100074768]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(trainX, trainY, verbose=0)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Best accuracy overall!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Neural Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save my NN model to YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code help from https://towardsdatascience.com/saving-and-loading-keras-model-42195b92f57a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to YAML\n",
    "model_yaml = model.to_yaml()\n",
    "with open(\"model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load YAML and create model\n",
    "yaml_file = open('model.yaml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 96.37%\n"
     ]
    }
   ],
   "source": [
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(trainX, trainY, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# save model and architecture to single file\n",
    "model.save(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next repeat this for each source phrase in a dataset and \n",
    "#-> compare the predicted result to the expected target phrase in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code help from #https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[ich bin ungeduldig], target=[im impatient], predicted=[im impatient]\n",
      "src=[wie seltsam], target=[how strange], predicted=[how strange]\n",
      "src=[ich bin nicht schuchtern], target=[im not shy], predicted=[im not shy]\n",
      "src=[ich brauche eine jacke], target=[i need a coat], predicted=[i need a coat]\n",
      "src=[ich habe tom getroffen], target=[i met tom], predicted=[ive seen tom]\n",
      "src=[ich habe tom verachtet], target=[i despised tom], predicted=[i despised tom]\n",
      "src=[wir sind sauber], target=[were clean], predicted=[were clean]\n",
      "src=[tom wurde gefeuert], target=[tom got fired], predicted=[tom was fired]\n",
      "src=[konnen sie mir folgen], target=[do you follow], predicted=[do you follow]\n",
      "src=[entlassen wir tom], target=[lets fire tom], predicted=[lets fire tom]\n",
      "BLEU-1: 0.945465\n",
      "BLEU-2: 0.927995\n",
      "BLEU-3: 0.841743\n",
      "BLEU-4: 0.513061\n",
      "test\n",
      "src=[das ist nicht okay], target=[thats not ok], predicted=[thats not ok]\n",
      "src=[das ist besser], target=[this is better], predicted=[thats better]\n",
      "src=[ich ergebe mich], target=[i surrender], predicted=[i fell me]\n",
      "src=[offne es bitte], target=[please open it], predicted=[please stop]\n",
      "src=[du hast mir gefehlt], target=[i missed you], predicted=[we know you]\n",
      "src=[mach dich fort], target=[go away], predicted=[beat away]\n",
      "src=[ist es ein geheimnis], target=[is it a secret], predicted=[is is is]\n",
      "src=[es ist nicht gut], target=[it isnt good], predicted=[its not good]\n",
      "src=[ich rannte nach drauen], target=[i ran outside], predicted=[i fired out]\n",
      "src=[es wird schlimmer], target=[it gets worse], predicted=[it gets worse]\n",
      "BLEU-1: 0.578846\n",
      "BLEU-2: 0.461705\n",
      "BLEU-3: 0.376376\n",
      "BLEU-4: 0.181873\n"
     ]
    }
   ],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    " \n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    " \n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    " \n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    " \n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    " \n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append([raw_target.split()])\n",
    "        predicted.append(translation.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    " \n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    " \n",
    "# load model\n",
    "model = load_model('model.h5')\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code help from https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-116-193c8672be89>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "model = load_model('model.h5')\n",
    "preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(n, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == n:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions into text (English)\n",
    "preds_text = []\n",
    "for i in preds:\n",
    "    temp = []\n",
    "    for j in range(len(i)):\n",
    "        t = get_word(i[j], eng_tokenizer)\n",
    "        if j > 0:\n",
    "            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)\n",
    "             \n",
    "        else:\n",
    "            if(t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)            \n",
    "        \n",
    "    preds_text.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thats not ok</td>\n",
       "      <td>thats not ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is better</td>\n",
       "      <td>thats better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i surrender</td>\n",
       "      <td>i fell me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>please open it</td>\n",
       "      <td>please stop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i missed you</td>\n",
       "      <td>we know you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>go away</td>\n",
       "      <td>beat away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>is it a secret</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>it isnt good</td>\n",
       "      <td>its not good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i ran outside</td>\n",
       "      <td>i fired out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>it gets worse</td>\n",
       "      <td>it gets worse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tom tried</td>\n",
       "      <td>tom tried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>said who</td>\n",
       "      <td>who says that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>who died</td>\n",
       "      <td>who sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hey wait up</td>\n",
       "      <td>hey wait up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>im here too</td>\n",
       "      <td>im too</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            actual        predicted\n",
       "0     thats not ok   thats not ok  \n",
       "1   this is better  thats better   \n",
       "2      i surrender      i fell me  \n",
       "3   please open it   please stop   \n",
       "4     i missed you    we know you  \n",
       "5          go away     beat away   \n",
       "6   is it a secret           is    \n",
       "7     it isnt good   its not good  \n",
       "8    i ran outside    i fired out  \n",
       "9    it gets worse  it gets worse  \n",
       "10       tom tried     tom tried   \n",
       "11        said who  who says that  \n",
       "12        who died     who sorry   \n",
       "13     hey wait up    hey wait up  \n",
       "14     im here too        im too   "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>are you alone</td>\n",
       "      <td>are you lonely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>tom is special</td>\n",
       "      <td>tom is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>keep singing</td>\n",
       "      <td>go on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>he was lucky</td>\n",
       "      <td>he was lucky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>take my car</td>\n",
       "      <td>no my car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>get on in here</td>\n",
       "      <td>come  inside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>tom sent me</td>\n",
       "      <td>tom beat me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>well survive</td>\n",
       "      <td>well will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>dont panic</td>\n",
       "      <td>dont hurt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>it happened</td>\n",
       "      <td>its happened</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>he resigned</td>\n",
       "      <td>he quit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>hes in pain</td>\n",
       "      <td>he is in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>are you bored</td>\n",
       "      <td>take yourself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>go home</td>\n",
       "      <td>go on home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>tom cheated</td>\n",
       "      <td>tom cheated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             actual         predicted\n",
       "469   are you alone  are you lonely  \n",
       "933  tom is special         tom is   \n",
       "405    keep singing          go on   \n",
       "459    he was lucky    he was lucky  \n",
       "406     take my car       no my car  \n",
       "510  get on in here    come  inside  \n",
       "714     tom sent me     tom beat me  \n",
       "557    well survive      well will   \n",
       "250      dont panic      dont hurt   \n",
       "396     it happened   its happened   \n",
       "902     he resigned        he quit   \n",
       "937     hes in pain        he is in  \n",
       "561   are you bored  take yourself   \n",
       "455         go home      go on home  \n",
       "125     tom cheated    tom cheated   "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
