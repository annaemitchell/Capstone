{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation\n",
    "### English to Italian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code help and adaption for this project come from the following resources:\n",
    "\n",
    "#https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/\n",
    "#https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
    "#https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "#https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/\n",
    "#https://google.github.io/seq2seq/nmt/\n",
    "#https://opennmt.net/\n",
    "#https://stackoverflow.com/questions/tagged/machine-translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "# load data to preserve unicode italian characters, loads file as a blob of text\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a loaded document into sentences\n",
    "def pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "# remove non printable characters\n",
    "# remove all punctuation characters\n",
    "# normalize all unicode characters to ASCII\n",
    "#normalize the case to lower\n",
    "#remove any remaining tokens that are not alphabetic \n",
    "#perform these operations on each phrase for each pair in the loaded dataset.\n",
    "\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Source: http://www.manythings.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-italian.pkl\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "filename = 'ita.txt'\n",
    "doc = load_doc(filename)\n",
    "# split into english-italian pairs\n",
    "pairs = pairs(doc)\n",
    "# clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "# save clean pairs to file\n",
    "clean_data(clean_pairs, 'english-italian.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hi] => [ciao]\n",
      "[run] => [corri]\n",
      "[run] => [corra]\n",
      "[run] => [correte]\n",
      "[who] => [chi]\n",
      "[wow] => [wow]\n",
      "[jump] => [salta]\n",
      "[jump] => [salti]\n",
      "[jump] => [saltate]\n",
      "[jump] => [salta]\n",
      "[jump] => [salti]\n",
      "[jump] => [saltate]\n",
      "[stop] => [fermati]\n",
      "[stop] => [fermatevi]\n",
      "[stop] => [stop]\n",
      "[stop] => [si fermi]\n",
      "[wait] => [aspetta]\n",
      "[wait] => [aspettate]\n",
      "[wait] => [aspetti]\n",
      "[wait] => [aspetta]\n",
      "[wait] => [aspetti]\n",
      "[wait] => [aspettate]\n",
      "[do it] => [fallo]\n",
      "[do it] => [falla]\n",
      "[do it] => [lo faccia]\n",
      "[do it] => [la faccia]\n",
      "[do it] => [fatelo]\n",
      "[do it] => [fatela]\n",
      "[go on] => [vai avanti]\n",
      "[go on] => [continua]\n",
      "[go on] => [continui]\n",
      "[go on] => [continuate]\n",
      "[go on] => [vada avanti]\n",
      "[go on] => [andate avanti]\n",
      "[hello] => [buongiorno]\n",
      "[hello] => [ciao]\n",
      "[hello] => [salve]\n",
      "[i ran] => [ho corso]\n",
      "[i ran] => [corsi]\n",
      "[i see] => [capisco]\n",
      "[i see] => [io capisco]\n",
      "[i try] => [provo]\n",
      "[i try] => [io provo]\n",
      "[i won] => [ho vinto]\n",
      "[i won] => [ho vinto]\n",
      "[i won] => [vinsi]\n",
      "[oh no] => [oh no]\n",
      "[relax] => [rilassati]\n",
      "[relax] => [si rilassi]\n",
      "[relax] => [rilassatevi]\n"
     ]
    }
   ],
   "source": [
    "# check if working\n",
    "for i in range(50):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-italian-both.pkl\n",
      "Saved: english-italian-train.pkl\n",
      "Saved: english-italian-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('english-italian.pkl')\n",
    "\n",
    "# reduce dataset size\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:9000], dataset[9000:]\n",
    "# save\n",
    "clean_data(dataset, 'english-italian-both.pkl')\n",
    "clean_data(train, 'english-italian-train.pkl')\n",
    "clean_data(test, 'english-italian-test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text to Sequence Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read raw text file\n",
    "def read_text(filename):\n",
    "    # open the file\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a text into sentences\n",
    "def to_lines(text):\n",
    "    sents = text.strip().split('\\n')\n",
    "    sents = [i.split('\\t') for i in sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_text(\"ita.txt\")\n",
    "ita_eng = to_lines(data)\n",
    "ita_eng = array(ita_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ita_eng = ita_eng[:50000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "ita_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in ita_eng[:,0]]\n",
    "ita_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in ita_eng[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lowercase\n",
    "for i in range(len(ita_eng)):\n",
    "    ita_eng[i,0] = ita_eng[i,0].lower()\n",
    "    \n",
    "    ita_eng[i,1] = ita_eng[i,1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists\n",
    "eng_l = []\n",
    "ita_l = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in ita_eng[:,0]:\n",
    "    ita_l.append(len(i.split()))\n",
    "\n",
    "for i in ita_eng[:,1]:\n",
    "    ita_l.append(len(i.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length_df = pd.DataFrame({'english':eng_l, 'italian':ita_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build a tokenizer\n",
    "def tokenization(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 4212\n"
     ]
    }
   ],
   "source": [
    "# english tokenizer\n",
    "eng_tokenizer = tokenization(ita_eng[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "eng_length = 10\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian Vocabulary Size: 9665\n"
     ]
    }
   ],
   "source": [
    "# italian tokenizer\n",
    "ita_tokenizer = tokenization(ita_eng[:, 1])\n",
    "ita_vocab_size = len(ita_tokenizer.word_index) + 1\n",
    "\n",
    "ita_length = 8\n",
    "print('Italian Vocabulary Size: %d' % ita_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#develop neural translation model\n",
    "#load and prepare clean text data ready for modeling and define and train the model\n",
    "#-> on the prepared data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code adapted from https://medium.com/@umerfarooq_26378/neural-machine-translation-with-code-68c425044bbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 1409\n",
      "English Max Length: 4\n",
      "Italian Vocabulary Size: 3594\n",
      "Italian Max Length: 7\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 7, 256)            920064    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 4, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 4, 1409)           362113    \n",
      "=================================================================\n",
      "Total params: 2,332,801\n",
      "Trainable params: 2,332,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.53561, saving model to model.h5\n",
      "141/141 - 8s - loss: 4.3179 - val_loss: 3.5356\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.53561 to 3.31408, saving model to model.h5\n",
      "141/141 - 6s - loss: 3.3776 - val_loss: 3.3141\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.31408 to 3.09710, saving model to model.h5\n",
      "141/141 - 6s - loss: 3.1315 - val_loss: 3.0971\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.09710 to 2.91334, saving model to model.h5\n",
      "141/141 - 6s - loss: 2.8778 - val_loss: 2.9133\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.91334 to 2.69016, saving model to model.h5\n",
      "141/141 - 6s - loss: 2.6286 - val_loss: 2.6902\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.69016 to 2.50787, saving model to model.h5\n",
      "141/141 - 6s - loss: 2.3749 - val_loss: 2.5079\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.50787 to 2.33382, saving model to model.h5\n",
      "141/141 - 6s - loss: 2.1460 - val_loss: 2.3338\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.33382 to 2.20082, saving model to model.h5\n",
      "141/141 - 6s - loss: 1.9367 - val_loss: 2.2008\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.20082 to 2.05648, saving model to model.h5\n",
      "141/141 - 6s - loss: 1.7583 - val_loss: 2.0565\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.05648 to 1.94752, saving model to model.h5\n",
      "141/141 - 6s - loss: 1.5912 - val_loss: 1.9475\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.94752 to 1.86023, saving model to model.h5\n",
      "141/141 - 6s - loss: 1.4346 - val_loss: 1.8602\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.86023 to 1.77860, saving model to model.h5\n",
      "141/141 - 6s - loss: 1.2940 - val_loss: 1.7786\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.77860 to 1.68760, saving model to model.h5\n",
      "141/141 - 6s - loss: 1.1676 - val_loss: 1.6876\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.68760 to 1.61497, saving model to model.h5\n",
      "141/141 - 6s - loss: 1.0463 - val_loss: 1.6150\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.61497 to 1.55748, saving model to model.h5\n",
      "141/141 - 6s - loss: 0.9378 - val_loss: 1.5575\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.55748 to 1.49546, saving model to model.h5\n",
      "141/141 - 6s - loss: 0.8376 - val_loss: 1.4955\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.49546 to 1.44903, saving model to model.h5\n",
      "141/141 - 6s - loss: 0.7488 - val_loss: 1.4490\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.44903 to 1.38638, saving model to model.h5\n",
      "141/141 - 6s - loss: 0.6644 - val_loss: 1.3864\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.38638 to 1.34608, saving model to model.h5\n",
      "141/141 - 6s - loss: 0.5910 - val_loss: 1.3461\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.34608 to 1.30281, saving model to model.h5\n",
      "141/141 - 6s - loss: 0.5253 - val_loss: 1.3028\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.30281 to 1.28837, saving model to model.h5\n",
      "141/141 - 6s - loss: 0.4691 - val_loss: 1.2884\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.28837 to 1.26175, saving model to model.h5\n",
      "141/141 - 6s - loss: 0.4198 - val_loss: 1.2617\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.26175 to 1.22942, saving model to model.h5\n",
      "141/141 - 6s - loss: 0.3796 - val_loss: 1.2294\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.22942 to 1.22432, saving model to model.h5\n",
      "141/141 - 6s - loss: 0.3418 - val_loss: 1.2243\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.22432 to 1.20096, saving model to model.h5\n",
      "141/141 - 6s - loss: 0.3093 - val_loss: 1.2010\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.20096 to 1.17822, saving model to model.h5\n",
      "141/141 - 6s - loss: 0.2814 - val_loss: 1.1782\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.17822\n",
      "141/141 - 6s - loss: 0.2578 - val_loss: 1.1832\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.17822 to 1.17170, saving model to model.h5\n",
      "141/141 - 6s - loss: 0.2388 - val_loss: 1.1717\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.17170\n",
      "141/141 - 6s - loss: 0.2223 - val_loss: 1.1758\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.17170 to 1.15979, saving model to model.h5\n",
      "141/141 - 6s - loss: 0.2099 - val_loss: 1.1598\n"
     ]
    }
   ],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    " \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    " \n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    " \n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    " \n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    " \n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-italian-both.pkl')\n",
    "train = load_clean_sentences('english-italian-train.pkl')\n",
    "test = load_clean_sentences('english-italian-test.pkl')\n",
    " \n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare italian tokenizer\n",
    "ita_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ita_vocab_size = len(ita_tokenizer.word_index) + 1\n",
    "ita_length = max_length(dataset[:, 1])\n",
    "print('Italian Vocabulary Size: %d' % ita_vocab_size)\n",
    "print('Italian Max Length: %d' % (ita_length))\n",
    " \n",
    "# prepare training data\n",
    "trainX = encode_sequences(ita_tokenizer, ita_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "# prepare validation data\n",
    "testX = encode_sequences(ita_tokenizer, ita_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    " \n",
    "# define model\n",
    "model = define_model(ita_vocab_size, eng_vocab_size, ita_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history = model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's compare the training loss and the validation loss.\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Neural Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-italian-both.pkl')\n",
    "train = load_clean_sentences('english-italian-train.pkl')\n",
    "test = load_clean_sentences('english-italian-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare italian tokenizer\n",
    "ita_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ita_vocab_size = len(ita_tokenizer.word_index) + 1\n",
    "ita_length = max_length(dataset[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "trainX = encode_sequences(ita_tokenizer, ita_length, train[:, 1])\n",
    "testX = encode_sequences(ita_tokenizer, ita_length, test[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save my NN model to YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code help from https://towardsdatascience.com/saving-and-loading-keras-model-42195b92f57a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17755618691444397"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(trainX, trainY, verbose=0)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to YAML\n",
    "model_yaml = model.to_yaml()\n",
    "with open(\"model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load YAML and create model\n",
    "yaml_file = open('model.yaml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 94.77%\n"
     ]
    }
   ],
   "source": [
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(trainX, trainY, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# save model and architecture to single file\n",
    "model.save(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next repeat this for each source phrase in a dataset and \n",
    "#-> compare the predicted result to the expected target phrase in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code help from #https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[vai al piano di sopra], target=[get upstairs], predicted=[get upstairs]\n",
      "src=[possiamo andare], target=[can we go], predicted=[we we go]\n",
      "src=[sappiamo il perche], target=[we know why], predicted=[we know why]\n",
      "src=[non parlare], target=[dont talk], predicted=[dont talk]\n",
      "src=[nessuno lo ha chiesto], target=[nobody asked], predicted=[nobody asked]\n",
      "src=[tom restera], target=[tomll stay], predicted=[tomll stay]\n",
      "src=[hanno rifiutato], target=[they refused], predicted=[they refused]\n",
      "src=[lui lha negata], target=[he denied it], predicted=[he denied it]\n",
      "src=[sii calmo], target=[be calm], predicted=[be calm]\n",
      "src=[non stare in piedi], target=[dont stand], predicted=[dont stand]\n",
      "BLEU-1: 0.932758\n",
      "BLEU-2: 0.905424\n",
      "BLEU-3: 0.720104\n",
      "BLEU-4: 0.284593\n",
      "test\n",
      "src=[vivo da sola], target=[i live alone], predicted=[i live alone]\n",
      "src=[state ferme], target=[stay put], predicted=[stand still]\n",
      "src=[non te la prendere], target=[take it easy], predicted=[dont not up]\n",
      "src=[come assurda], target=[how absurd], predicted=[how deep]\n",
      "src=[sono vecchio], target=[i am old], predicted=[im old]\n",
      "src=[lei e anziano], target=[youre old], predicted=[youre old]\n",
      "src=[state calmi], target=[keep calm], predicted=[sit still]\n",
      "src=[mi sbrighero], target=[ill hurry], predicted=[ill hurry]\n",
      "src=[non correro], target=[i wont run], predicted=[dont yell]\n",
      "src=[io vedro tom], target=[ill see tom], predicted=[ill see tom]\n",
      "BLEU-1: 0.717079\n",
      "BLEU-2: 0.647478\n",
      "BLEU-3: 0.519033\n",
      "BLEU-4: 0.217437\n"
     ]
    }
   ],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    " \n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    " \n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    " \n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    " \n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    " \n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append([raw_target.split()])\n",
    "        predicted.append(translation.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    " \n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-italian-both.pkl')\n",
    "train = load_clean_sentences('english-italian-train.pkl')\n",
    "test = load_clean_sentences('english-italian-test.pkl')\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "# prepare italian tokenizer\n",
    "ita_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ita_vocab_size = len(ita_tokenizer.word_index) + 1\n",
    "ita_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "trainX = encode_sequences(ita_tokenizer, ita_length, train[:, 1])\n",
    "testX = encode_sequences(ita_tokenizer, ita_length, test[:, 1])\n",
    " \n",
    "# load model\n",
    "model = load_model('model.h5')\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code help from https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-66-193c8672be89>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "model = load_model('model.h5')\n",
    "preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(n, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == n:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions into text (English)\n",
    "preds_text = []\n",
    "for i in preds:\n",
    "    temp = []\n",
    "    for j in range(len(i)):\n",
    "        t = get_word(i[j], eng_tokenizer)\n",
    "        if j > 0:\n",
    "            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)\n",
    "             \n",
    "        else:\n",
    "            if(t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)            \n",
    "        \n",
    "    preds_text.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i live alone</td>\n",
       "      <td>i live alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stay put</td>\n",
       "      <td>stand still</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>take it easy</td>\n",
       "      <td>dont not up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how absurd</td>\n",
       "      <td>how deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am old</td>\n",
       "      <td>im old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>youre old</td>\n",
       "      <td>youre old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>keep calm</td>\n",
       "      <td>sit still</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ill hurry</td>\n",
       "      <td>ill hurry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i wont run</td>\n",
       "      <td>dont yell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ill see tom</td>\n",
       "      <td>ill see tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>im diligent</td>\n",
       "      <td>im diligent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>youre wise</td>\n",
       "      <td>youre wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>who hit tom</td>\n",
       "      <td>who tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>im blind</td>\n",
       "      <td>im blind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>stay there</td>\n",
       "      <td>stay there</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          actual      predicted\n",
       "0   i live alone  i live alone \n",
       "1       stay put  stand still  \n",
       "2   take it easy   dont not up \n",
       "3     how absurd     how deep  \n",
       "4       i am old       im old  \n",
       "5      youre old    youre old  \n",
       "6      keep calm    sit still  \n",
       "7      ill hurry    ill hurry  \n",
       "8     i wont run    dont yell  \n",
       "9    ill see tom   ill see tom \n",
       "10   im diligent  im diligent  \n",
       "11    youre wise   youre wise  \n",
       "12   who hit tom      who tom  \n",
       "13      im blind     im blind  \n",
       "14    stay there   stay there  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>im a model</td>\n",
       "      <td>im a model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>im not sure</td>\n",
       "      <td>im not sure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>jump across</td>\n",
       "      <td>jump across</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>grab that</td>\n",
       "      <td>grab that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>be realistic</td>\n",
       "      <td>be realistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>i love kids</td>\n",
       "      <td>i love kids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>smile</td>\n",
       "      <td>jump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>take a card</td>\n",
       "      <td>take a card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>keep quiet</td>\n",
       "      <td>keep quiet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>are we ready</td>\n",
       "      <td>were ready</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>learn french</td>\n",
       "      <td>speak up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>i need a map</td>\n",
       "      <td>i need a map</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>fill it up</td>\n",
       "      <td>fill is art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>talk to me</td>\n",
       "      <td>talk to me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>let me out</td>\n",
       "      <td>let me out</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           actual       predicted\n",
       "761    im a model     im a model \n",
       "287   im not sure    im not sure \n",
       "226   jump across   jump across  \n",
       "732     grab that     grab that  \n",
       "499  be realistic  be realistic  \n",
       "545   i love kids    i love kids \n",
       "331         smile         jump   \n",
       "649   take a card    take a card \n",
       "74     keep quiet    keep quiet  \n",
       "142  are we ready    were ready  \n",
       "156  learn french      speak up  \n",
       "192  i need a map    i need a map\n",
       "36     fill it up    fill is art \n",
       "25     talk to me     talk to me \n",
       "235    let me out     let me out "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
