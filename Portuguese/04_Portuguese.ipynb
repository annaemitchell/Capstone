{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation\n",
    "### English to Portuguese  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code help and adaption for this project come from the following resources:\n",
    "\n",
    "#https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/\n",
    "#https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
    "#https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "#https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/\n",
    "#https://google.github.io/seq2seq/nmt/\n",
    "#https://opennmt.net/\n",
    "#https://stackoverflow.com/questions/tagged/machine-translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "# load data to preserve unicode characters, loads file as a blob of text\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a loaded document into sentences\n",
    "def pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "# remove non printable characters\n",
    "# remove all punctuation characters\n",
    "# normalize all unicode characters to ASCII\n",
    "#normalize the case to lower\n",
    "#remove any remaining tokens that are not alphabetic \n",
    "#perform these operations on each phrase for each pair in the loaded dataset.\n",
    "\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-port.pkl\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "filename = 'port.txt'\n",
    "doc = load_doc(filename)\n",
    "# split into english-port pairs\n",
    "pairs = pairs(doc)\n",
    "# clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "# save clean pairs to file\n",
    "clean_data(clean_pairs, 'english-port.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[go] => [vai]\n",
      "[go] => [va]\n",
      "[go] => [ir]\n",
      "[hi] => [oi]\n",
      "[run] => [corre]\n",
      "[run] => [corra]\n",
      "[run] => [corram]\n",
      "[run] => [corre]\n",
      "[run] => [corra]\n",
      "[run] => [corram]\n",
      "[who] => [quem]\n",
      "[who] => [que]\n",
      "[wow] => [uau]\n",
      "[wow] => [nossa]\n",
      "[wow] => [wow]\n",
      "[fire] => [fogo]\n",
      "[fire] => [incendio]\n",
      "[fire] => [chama]\n",
      "[help] => [ajuda]\n",
      "[help] => [socorro]\n",
      "[jump] => [pule]\n",
      "[jump] => [pula]\n",
      "[jump] => [pulem]\n",
      "[jump] => [pule]\n",
      "[jump] => [pulam]\n",
      "[stop] => [pare]\n",
      "[stop] => [parem]\n",
      "[stop] => [parada]\n",
      "[stop] => [ponto]\n",
      "[stop] => [para]\n",
      "[wait] => [espere]\n",
      "[wait] => [aguarde]\n",
      "[wait] => [espere]\n",
      "[wait] => [esperem]\n",
      "[go on] => [siga em frente]\n",
      "[go on] => [va]\n",
      "[go on] => [continue]\n",
      "[go on] => [siga adiante]\n",
      "[hello] => [oi]\n",
      "[hello] => [alo]\n",
      "[hello] => [ola]\n",
      "[hello] => [alo]\n",
      "[i ran] => [eu corri]\n",
      "[i see] => [eu sei]\n",
      "[i see] => [eu entendo]\n",
      "[i see] => [estou vendo]\n",
      "[i see] => [eu vejo]\n",
      "[i see] => [eu assisto]\n",
      "[i try] => [eu tento]\n",
      "[i try] => [tento]\n"
     ]
    }
   ],
   "source": [
    "# check if working\n",
    "for i in range(50):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-port-both.pkl\n",
      "Saved: english-port-train.pkl\n",
      "Saved: english-port-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('english-port.pkl')\n",
    "\n",
    "# reduce dataset size\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:9000], dataset[9000:]\n",
    "# save\n",
    "clean_data(dataset, 'english-port-both.pkl')\n",
    "clean_data(train, 'english-port-train.pkl')\n",
    "clean_data(test, 'english-port-test.pkl')\n",
    "\n",
    "#both.pkl - contains all of the train and test examples that we can use to \n",
    "#-> define the parameters of the problem, such a smax phrase lengths and the vocab\n",
    "# train.pkl and test.pkl - train and test dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text to Sequence Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read raw text file\n",
    "def read_text(filename):\n",
    "    # open the file\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a text into sentences\n",
    "def to_lines(text):\n",
    "    sents = text.strip().split('\\n')\n",
    "    sents = [i.split('\\t') for i in sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_text(\"port.txt\")\n",
    "port_eng = to_lines(data)\n",
    "port_eng = array(port_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_eng = port_eng[:50000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "port_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in port_eng[:,0]]\n",
    "port_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in port_eng[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lowercase\n",
    "for i in range(len(port_eng)):\n",
    "    port_eng[i,0] = port_eng[i,0].lower()\n",
    "    \n",
    "    port_eng[i,1] = port_eng[i,1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists\n",
    "eng_l = []\n",
    "port_l = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in port_eng[:,0]:\n",
    "    port_l.append(len(i.split()))\n",
    "\n",
    "for i in port_eng[:,1]:\n",
    "    port_l.append(len(i.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length_df = pd.DataFrame({'english':eng_l, 'portuguese':port_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build a tokenizer\n",
    "def tokenization(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 12772\n"
     ]
    }
   ],
   "source": [
    "# english tokenizer\n",
    "eng_tokenizer = tokenization(port_eng[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "eng_length = 8\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portuguese Vocabulary Size: 10067\n"
     ]
    }
   ],
   "source": [
    "# Port tokenizer\n",
    "port_tokenizer = tokenization(port_eng[:, 1])\n",
    "port_vocab_size = len(port_tokenizer.word_index) + 1\n",
    "\n",
    "port_length = 8\n",
    "print('Portuguese Vocabulary Size: %d' % port_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#develop neural translation model\n",
    "#load and prepare clean text data ready for modeling and define and train the model\n",
    "#-> on the prepared data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code adapted from https://medium.com/@umerfarooq_26378/neural-machine-translation-with-code-68c425044bbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2083\n",
      "English Max Length: 5\n",
      "Portuguese Vocabulary Size: 3688\n",
      "Portuguese Max Length: 8\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 8, 256)            944128    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 5, 2083)           535331    \n",
      "=================================================================\n",
      "Total params: 2,530,083\n",
      "Trainable params: 2,530,083\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.38988, saving model to model.h5\n",
      "141/141 - 8s - loss: 4.1567 - val_loss: 3.3899\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.38988 to 3.22910, saving model to model.h5\n",
      "141/141 - 7s - loss: 3.1909 - val_loss: 3.2291\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.22910 to 3.14352, saving model to model.h5\n",
      "141/141 - 7s - loss: 3.0436 - val_loss: 3.1435\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.14352 to 3.06381, saving model to model.h5\n",
      "141/141 - 7s - loss: 2.9273 - val_loss: 3.0638\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.06381 to 2.96425, saving model to model.h5\n",
      "141/141 - 7s - loss: 2.8126 - val_loss: 2.9643\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.96425 to 2.84735, saving model to model.h5\n",
      "141/141 - 7s - loss: 2.6740 - val_loss: 2.8473\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.84735 to 2.70942, saving model to model.h5\n",
      "141/141 - 7s - loss: 2.4940 - val_loss: 2.7094\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.70942 to 2.58181, saving model to model.h5\n",
      "141/141 - 7s - loss: 2.3173 - val_loss: 2.5818\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.58181 to 2.46957, saving model to model.h5\n",
      "141/141 - 7s - loss: 2.1587 - val_loss: 2.4696\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.46957 to 2.37449, saving model to model.h5\n",
      "141/141 - 7s - loss: 2.0080 - val_loss: 2.3745\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.37449 to 2.29065, saving model to model.h5\n",
      "141/141 - 7s - loss: 1.8734 - val_loss: 2.2906\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.29065 to 2.21220, saving model to model.h5\n",
      "141/141 - 7s - loss: 1.7442 - val_loss: 2.2122\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.21220 to 2.14870, saving model to model.h5\n",
      "141/141 - 7s - loss: 1.6256 - val_loss: 2.1487\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.14870 to 2.08511, saving model to model.h5\n",
      "141/141 - 8s - loss: 1.5108 - val_loss: 2.0851\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.08511 to 2.04456, saving model to model.h5\n",
      "141/141 - 7s - loss: 1.4049 - val_loss: 2.0446\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.04456 to 2.00136, saving model to model.h5\n",
      "141/141 - 7s - loss: 1.3059 - val_loss: 2.0014\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.00136 to 1.94933, saving model to model.h5\n",
      "141/141 - 7s - loss: 1.2092 - val_loss: 1.9493\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.94933 to 1.92729, saving model to model.h5\n",
      "141/141 - 7s - loss: 1.1195 - val_loss: 1.9273\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.92729 to 1.89694, saving model to model.h5\n",
      "141/141 - 7s - loss: 1.0311 - val_loss: 1.8969\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.89694 to 1.87743, saving model to model.h5\n",
      "141/141 - 8s - loss: 0.9515 - val_loss: 1.8774\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.87743 to 1.84648, saving model to model.h5\n",
      "141/141 - 7s - loss: 0.8762 - val_loss: 1.8465\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.84648 to 1.81888, saving model to model.h5\n",
      "141/141 - 7s - loss: 0.8046 - val_loss: 1.8189\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.81888 to 1.80446, saving model to model.h5\n",
      "141/141 - 7s - loss: 0.7377 - val_loss: 1.8045\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.80446 to 1.79648, saving model to model.h5\n",
      "141/141 - 7s - loss: 0.6797 - val_loss: 1.7965\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.79648 to 1.78125, saving model to model.h5\n",
      "141/141 - 7s - loss: 0.6228 - val_loss: 1.7812\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.78125 to 1.75018, saving model to model.h5\n",
      "141/141 - 7s - loss: 0.5725 - val_loss: 1.7502\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.75018 to 1.74656, saving model to model.h5\n",
      "141/141 - 7s - loss: 0.5251 - val_loss: 1.7466\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.74656\n",
      "141/141 - 7s - loss: 0.4838 - val_loss: 1.7519\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.74656 to 1.73264, saving model to model.h5\n",
      "141/141 - 7s - loss: 0.4459 - val_loss: 1.7326\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.73264 to 1.72758, saving model to model.h5\n",
      "141/141 - 7s - loss: 0.4107 - val_loss: 1.7276\n"
     ]
    }
   ],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    " \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    " \n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    " \n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    " \n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    " \n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    " \n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-port-both.pkl')\n",
    "train = load_clean_sentences('english-port-train.pkl')\n",
    "test = load_clean_sentences('english-port-test.pkl')\n",
    " \n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare port tokenizer\n",
    "port_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "port_vocab_size = len(port_tokenizer.word_index) + 1\n",
    "port_length = max_length(dataset[:, 1])\n",
    "print('Portuguese Vocabulary Size: %d' % port_vocab_size)\n",
    "print('Portuguese Max Length: %d' % (port_length))\n",
    " \n",
    "# prepare training data\n",
    "trainX = encode_sequences(port_tokenizer, port_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "# prepare validation data\n",
    "testX = encode_sequences(port_tokenizer, port_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    " \n",
    "# define model\n",
    "model = define_model(port_vocab_size, eng_vocab_size, port_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history = model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's compare the training loss and the validation loss.\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Neural Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-port-both.pkl')\n",
    "train = load_clean_sentences('english-port-train.pkl')\n",
    "test = load_clean_sentences('english-port-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare port tokenizer\n",
    "port_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "port_vocab_size = len(port_tokenizer.word_index) + 1\n",
    "port_length = max_length(dataset[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "trainX = encode_sequences(port_tokenizer, port_length, train[:, 1])\n",
    "testX = encode_sequences(port_tokenizer, port_length, test[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save my NN model to YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code help from https://towardsdatascience.com/saving-and-loading-keras-model-42195b92f57a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3503669798374176"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(trainX, trainY, verbose=0)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to YAML\n",
    "model_yaml = model.to_yaml()\n",
    "with open(\"model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load YAML and create model\n",
    "yaml_file = open('model.yaml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 91.84%\n"
     ]
    }
   ],
   "source": [
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(trainX, trainY, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#save model and architecture to single file\n",
    "model.save(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next repeat this for each source phrase in a dataset and \n",
    "#-> compare the predicted result to the expected target phrase in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code help from #https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[isso nao funcionara], target=[it wont work], predicted=[it wont work]\n",
      "src=[eu gostei de tom], target=[i liked tom], predicted=[i liked tom]\n",
      "src=[eles confiam em tom], target=[they trust tom], predicted=[they trust tom]\n",
      "src=[dirija rapido], target=[drive faster], predicted=[drive faster]\n",
      "src=[eu sou cego], target=[im blind], predicted=[im blind]\n",
      "src=[quem tem], target=[who has it], predicted=[who has it]\n",
      "src=[para de atirar], target=[stop shooting], predicted=[stop shooting]\n",
      "src=[tom nos avisou], target=[tom warned us], predicted=[tom warned us]\n",
      "src=[gosto muito do tom], target=[i do like tom], predicted=[i do like tom]\n",
      "src=[eles encontraram], target=[they found it], predicted=[they found it]\n",
      "BLEU-1: 0.878581\n",
      "BLEU-2: 0.824619\n",
      "BLEU-3: 0.722266\n",
      "BLEU-4: 0.404552\n",
      "test\n",
      "src=[chame a seguranca], target=[call security], predicted=[call security]\n",
      "src=[tentao], target=[try it], predicted=[take happens]\n",
      "src=[e mesmo], target=[is that so], predicted=[its]\n",
      "src=[eu gosto de costurar], target=[i like sewing], predicted=[i like apples]\n",
      "src=[estou muito cansado], target=[im very tired], predicted=[im very tired]\n",
      "src=[eu vou te processar], target=[ill sue you], predicted=[i will sue you]\n",
      "src=[eu vou voltar], target=[ill go back], predicted=[i will return]\n",
      "src=[vamos dancar], target=[shall we dance], predicted=[well dance]\n",
      "src=[eu sou diabetico], target=[im a diabetic], predicted=[im diabetic]\n",
      "src=[eu nao vou morder], target=[i wont bite], predicted=[i wont bite]\n",
      "BLEU-1: 0.578231\n",
      "BLEU-2: 0.462546\n",
      "BLEU-3: 0.383442\n",
      "BLEU-4: 0.190543\n"
     ]
    }
   ],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    " \n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    " \n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    " \n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    " \n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    " \n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append([raw_target.split()])\n",
    "        predicted.append(translation.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    " \n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-port-both.pkl')\n",
    "train = load_clean_sentences('english-port-train.pkl')\n",
    "test = load_clean_sentences('english-port-test.pkl')\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "# prepare port tokenizer\n",
    "port_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "port_vocab_size = len(port_tokenizer.word_index) + 1\n",
    "port_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "trainX = encode_sequences(port_tokenizer, port_length, train[:, 1])\n",
    "testX = encode_sequences(port_tokenizer, port_length, test[:, 1])\n",
    " \n",
    "# load model\n",
    "model = load_model('model.h5')\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code help from https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')\n",
    "preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(n, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == n:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions into text (English)\n",
    "preds_text = []\n",
    "for i in preds:\n",
    "    temp = []\n",
    "    for j in range(len(i)):\n",
    "        t = get_word(i[j], eng_tokenizer)\n",
    "        if j > 0:\n",
    "            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)\n",
    "             \n",
    "        else:\n",
    "            if(t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)            \n",
    "        \n",
    "    preds_text.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>call security</td>\n",
       "      <td>call security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>try it</td>\n",
       "      <td>take happens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is that so</td>\n",
       "      <td>its</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i like sewing</td>\n",
       "      <td>i like apples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im very tired</td>\n",
       "      <td>im very tired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ill sue you</td>\n",
       "      <td>i will sue you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ill go back</td>\n",
       "      <td>i will return</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>shall we dance</td>\n",
       "      <td>well dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>im a diabetic</td>\n",
       "      <td>im diabetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i wont bite</td>\n",
       "      <td>i wont bite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i drink coffee</td>\n",
       "      <td>i drink coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tom loves that</td>\n",
       "      <td>tom likes it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tom likes mary</td>\n",
       "      <td>tom likes mary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ill do it now</td>\n",
       "      <td>ill pay this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>i got promoted</td>\n",
       "      <td>i was angry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            actual         predicted\n",
       "0    call security  call security   \n",
       "1           try it   take happens   \n",
       "2       is that so           its    \n",
       "3    i like sewing   i like apples  \n",
       "4    im very tired   im very tired  \n",
       "5      ill sue you   i will sue you \n",
       "6      ill go back   i will return  \n",
       "7   shall we dance     well dance   \n",
       "8    im a diabetic    im diabetic   \n",
       "9      i wont bite     i wont bite  \n",
       "10  i drink coffee  i drink coffee  \n",
       "11  tom loves that    tom likes it  \n",
       "12  tom likes mary  tom likes mary  \n",
       "13   ill do it now    ill pay this  \n",
       "14  i got promoted     i was angry  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>tom won</td>\n",
       "      <td>tom won</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>i said id go</td>\n",
       "      <td>i said go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>someone called</td>\n",
       "      <td>get up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>well fight</td>\n",
       "      <td>we care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>no one came</td>\n",
       "      <td>nobody came</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>im curious</td>\n",
       "      <td>i am curious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>tom has to pay</td>\n",
       "      <td>tom has a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>i keep a dog</td>\n",
       "      <td>i have a dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>no ones here</td>\n",
       "      <td>its not here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>tom knows</td>\n",
       "      <td>tom  try</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>they were weak</td>\n",
       "      <td>they were weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>help me</td>\n",
       "      <td>help me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>go away</td>\n",
       "      <td>get out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>it was a bet</td>\n",
       "      <td>it was a sale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>where are you</td>\n",
       "      <td>are  you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             actual         predicted\n",
       "147         tom won        tom won   \n",
       "681    i said id go       i said go  \n",
       "182  someone called         get up   \n",
       "632      well fight        we care   \n",
       "559     no one came    nobody came   \n",
       "812      im curious    i am curious  \n",
       "793  tom has to pay       tom has a  \n",
       "605    i keep a dog     i have a dog \n",
       "628    no ones here    its not here  \n",
       "802       tom knows        tom  try  \n",
       "835  they were weak  they were weak  \n",
       "92          help me        help me   \n",
       "820         go away        get out   \n",
       "294    it was a bet    it was a sale \n",
       "364   where are you        are  you  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
